{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0513dee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59108,
     "status": "ok",
     "timestamp": 1678785191815,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "0513dee4",
    "outputId": "39a1d58e-c297-4d82-e02a-7df8373ffaf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.26.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting textract\n",
      "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
      "Collecting argcomplete~=1.10.0\n",
      "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting six~=1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdfminer.six==20191110\n",
      "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-pptx~=0.6.18\n",
      "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting beautifulsoup4~=4.8.0\n",
      "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docx2txt~=0.8\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting SpeechRecognition~=3.8.1\n",
      "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xlrd~=1.2.0\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting extract-msg<=0.29.*\n",
      "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.9/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.4)\n",
      "Collecting compressed-rtf>=1.0.6\n",
      "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tzlocal>=2.1 in /usr/local/lib/python3.9/dist-packages (from extract-msg<=0.29.*->textract) (4.2)\n",
      "Collecting imapclient==2.1.0\n",
      "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ebcdic>=1.1.1\n",
      "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting olefile>=0.46\n",
      "  Downloading olefile-0.46.zip (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from python-pptx~=0.6.18->textract) (4.9.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.9/dist-packages (from python-pptx~=0.6.18->textract) (8.4.0)\n",
      "Collecting XlsxWriter>=0.5.7\n",
      "  Downloading XlsxWriter-3.0.9-py3-none-any.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=2.1->extract-msg<=0.29.*->textract) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=2.1->extract-msg<=0.29.*->textract) (2022.7)\n",
      "Building wheels for collected packages: docx2txt, python-pptx, compressed-rtf, olefile\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=de73d45e9de4a01265f52c6eb28ab41a61f5e1d5321f6049e9f1b93450f12018\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/75/01/e6c444034338bde9c7947d3467807f889123465c2371e77418\n",
      "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470952 sha256=b4bc1c5f57e1355da8cee583b9d150e35f181e0c5c10557fd0c6f6ae8720b50c\n",
      "  Stored in directory: /root/.cache/pip/wheels/0e/4a/ed/9653bc799915f52dce3f04d14946fbd85cce9c3cdedc9cfa71\n",
      "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6200 sha256=4a21c66083938709e9b1c2a4c3758777537f4a1a6c0bcb5f75d9ec400260a2a2\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/67/e4/ba2159853bdd0fe99330aa1e384915108143a5370686ea446f\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35430 sha256=d17d5a14aeaeb749c21bee9cf78d8dc8f1a216f516676cd65271bae501b89b4b\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/b8/ba/ebba30390fbd997074f35e42a842ce3fd933213cac8753414e\n",
      "Successfully built docx2txt python-pptx compressed-rtf olefile\n",
      "Installing collected packages: SpeechRecognition, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, six, pycryptodome, olefile, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: xlrd\n",
      "    Found existing installation: xlrd 2.0.1\n",
      "    Uninstalling xlrd-2.0.1:\n",
      "      Successfully uninstalled xlrd-2.0.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.9.3\n",
      "    Uninstalling beautifulsoup4-4.9.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.9.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SpeechRecognition-3.8.1 XlsxWriter-3.0.9 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.17 python-pptx-0.6.21 six-1.12.0 textract-1.6.5 xlrd-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install transformers\n",
    "!pip install PyPDF2\n",
    "!pip install textract\n",
    "import json \n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd \n",
    "import string\n",
    "# normalize nested JSON files\n",
    "from pandas.io.json import json_normalize\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2, urllib.request , nltk , textract\n",
    "from io import BytesIO\n",
    "import json\n",
    "#import weasyprint\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "from fuzzywuzzy import fuzz\n",
    "import json\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import regex\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('stopwords')  \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "HebnOIf4whG5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23369,
     "status": "ok",
     "timestamp": 1678785242593,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "HebnOIf4whG5",
    "outputId": "5038d4af-15e7-47b2-d4c9-b87401018923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20de7fd",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1678785279649,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "b20de7fd"
   },
   "outputs": [],
   "source": [
    "JSON_FEATURE_DIR = \"/content/drive/MyDrive/RAjobs/try\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2wYZ74zCta1w",
   "metadata": {
    "id": "2wYZ74zCta1w"
   },
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "MfnH7Gv7wz0G",
   "metadata": {
    "executionInfo": {
     "elapsed": 601,
     "status": "ok",
     "timestamp": 1678785538854,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "MfnH7Gv7wz0G"
   },
   "outputs": [],
   "source": [
    "def content(jsonfile):\n",
    "    curr_id = jsonfile.split('.')[0]\n",
    "    with open(os.path.join(JSON_FEATURE_DIR, jsonfile)) as f:\n",
    "        raw_json = json.load(f)\n",
    "\n",
    "        ## for new data\n",
    "        raw_json = eval(\"[\"+raw_json['text']+\"]\")  ## convert str to dict\n",
    "        raw_json = [{'section_title': x['header'], 'text': x['text']} for x in raw_json]  ## clean data\n",
    "        ## for new data\n",
    "        \n",
    "        raw_text = list(map(lambda x: ' '.join(x.values()), raw_json))\n",
    "        raw_text = ' '.join(raw_text)\n",
    "        raw_text = ' '.join(raw_text.split())\n",
    "    sentences = '\\n'.join(sent_tokenize(raw_text))\n",
    "    return curr_id, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "FkLLZL7Zw5fk",
   "metadata": {
    "executionInfo": {
     "elapsed": 69621,
     "status": "ok",
     "timestamp": 1678785610780,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "FkLLZL7Zw5fk"
   },
   "outputs": [],
   "source": [
    "with mp.Pool(4) as pooler:\n",
    "    content_dicts = list(pooler.map(content, os.listdir(JSON_FEATURE_DIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "BWQApvGHw6vk",
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1678785633260,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "BWQApvGHw6vk"
   },
   "outputs": [],
   "source": [
    "content_df = pd.DataFrame(content_dicts)\n",
    "content_df = content_df.rename(columns={content_df.columns[0]: \"ID\",content_df.columns[1]: \"content\"})\n",
    "content_list = content_df['content'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sfGPP81mxCmK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1678785635486,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "sfGPP81mxCmK",
    "outputId": "b008520c-ca8a-4e3d-871f-1165c1ac642a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "from nltk import SnowballStemmer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Xhg4ZCKmxHs8",
   "metadata": {
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1678785637803,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "Xhg4ZCKmxHs8"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenize(text):\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))  \n",
    "    return [stemmer.stem(i) for i in text.translate(translator).split()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5C4lTZXsxfRX",
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1678785640945,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "5C4lTZXsxfRX"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = [tokenize(s)[0] for s in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "roc0L3l0xKlc",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678785642625,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "roc0L3l0xKlc"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",        \n",
    "                            tokenizer=tokenize,      \n",
    "                            ngram_range=(0,1),       \n",
    "                            strip_accents='unicode', \n",
    "                            stop_words = stop_words,\n",
    "                            min_df = 0.05,           \n",
    "                            max_df = 0.95)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd658d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33122,
     "status": "ok",
     "timestamp": 1678785677030,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "Ucw2pnecxNiA",
    "outputId": "d22b1a67-9b8d-49fb-e442-4433c2328788"
   },
   "outputs": [],
   "source": [
    "\n",
    "bag_of_words = vectorizer.fit_transform(content_list) \n",
    "features = vectorizer.get_feature_names_out()               \n",
    "\n",
    "\n",
    "transformer = TfidfTransformer(norm = None, smooth_idf = True, sublinear_tf = True)\n",
    "tfidf = transformer.fit_transform(bag_of_words)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = 5, learning_method='online')\n",
    "doctopic = lda.fit_transform(bag_of_words)\n",
    "\n",
    "\n",
    "ls_keywords = []\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:10]\n",
    "    keywords = ', '.join(features[i] for i in word_idx)\n",
    "    ls_keywords.append(keywords)\n",
    "    print(i, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d37ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1678785682642,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "yYCq4FZzxlPP",
    "outputId": "2b1c3683-2767-4cfa-d8a7-c91b977c9acc"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(doctopic, columns = ls_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae718d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1678785686541,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "YFKAgKVaxmFf",
    "outputId": "7727b5d6-243c-424f-a05a-878f38c187b8"
   },
   "outputs": [],
   "source": [
    "topics_doc = pd.DataFrame(doctopic, columns = ls_keywords)\n",
    "doc_topics_content_id = pd.concat([topics_doc,content_df],axis=1)\n",
    "doc_topics_content_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "FXEKKPc_xtK6",
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1678785691412,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "FXEKKPc_xtK6"
   },
   "outputs": [],
   "source": [
    "doc_topics_project_id_with_max = pd.concat([topics_doc.idxmax(axis=1),content_df],axis=1).rename(columns={0:'topic'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "s2J63dhxxyBv",
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1678785712164,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "s2J63dhxxyBv"
   },
   "outputs": [],
   "source": [
    "topic_group = doc_topics_project_id_with_max.groupby(\"topic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86824e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1678785742864,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "B3Tk8MBzbUmK",
    "outputId": "0010f11d-0c46-4099-87fb-97494c1c0858"
   },
   "outputs": [],
   "source": [
    "topic_group.get_group('oregon, manag, use, product, research, anim, studi, feed, forest, fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf2a93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1678785793333,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "SVNmxw3ObedZ",
    "outputId": "0dc5e653-9681-4a99-a62b-2088cc63d831"
   },
   "outputs": [],
   "source": [
    "topic_group.get_group('4, h, youth, project, counti, program, learn, member, scienc, introduct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3db38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1678785815384,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "FiPj2G80blod",
    "outputId": "00b83793-bd81-40f3-ab9c-a536d442a818"
   },
   "outputs": [],
   "source": [
    "topic_group.get_group('plant, soil, water, use, seed, fruit, dri, 1, grow, crop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309aee7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1678785849259,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "vjMQ869Sbro3",
    "outputId": "68265ac4-f84a-4a19-e48a-5571e2554fa0"
   },
   "outputs": [],
   "source": [
    "topic_group.get_group('tree, plant, expert, garden, ask, get, oregon, bee, answer, extens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a6e2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1678785878033,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "JDv3J5zzbyzP",
    "outputId": "79bc2246-b854-4679-857f-e2b240956f25"
   },
   "outputs": [],
   "source": [
    "topic_group.get_group('oregon, extens, garden, program, communiti, osu, food, counti, master, state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "BggtsgJkcE0y",
   "metadata": {
    "executionInfo": {
     "elapsed": 2980,
     "status": "ok",
     "timestamp": 1678785934205,
     "user": {
      "displayName": "Xiangyu Ren",
      "userId": "16178773805667225071"
     },
     "user_tz": 240
    },
    "id": "BggtsgJkcE0y"
   },
   "outputs": [],
   "source": [
    "doc_topics_project_id_with_max.to_excel('/content/drive/MyDrive/RAjobs/textanalysis_OSU.xlsx' ,index= False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
