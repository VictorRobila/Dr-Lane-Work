{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jheoZKI7B9K",
    "outputId": "c9dedab1-1b09-47a7-e4c5-5f223c48c914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting textract\n",
      "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
      "Collecting SpeechRecognition~=3.8.1\n",
      "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting beautifulsoup4~=4.8.0\n",
      "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xlrd~=1.2.0\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting argcomplete~=1.10.0\n",
      "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdfminer.six==20191110\n",
      "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting six~=1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting docx2txt~=0.8\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting python-pptx~=0.6.18\n",
      "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting extract-msg<=0.29.*\n",
      "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.9/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.4)\n",
      "Collecting compressed-rtf>=1.0.6\n",
      "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting olefile>=0.46\n",
      "  Downloading olefile-0.46.zip (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting imapclient==2.1.0\n",
      "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ebcdic>=1.1.1\n",
      "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tzlocal>=2.1 in /usr/local/lib/python3.9/dist-packages (from extract-msg<=0.29.*->textract) (4.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from python-pptx~=0.6.18->textract) (4.9.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.9/dist-packages (from python-pptx~=0.6.18->textract) (8.4.0)\n",
      "Collecting XlsxWriter>=0.5.7\n",
      "  Downloading XlsxWriter-3.0.9-py3-none-any.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=2.1->extract-msg<=0.29.*->textract) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=2.1->extract-msg<=0.29.*->textract) (2022.7)\n",
      "Building wheels for collected packages: docx2txt, python-pptx, compressed-rtf, olefile\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3977 sha256=e568e80c3adf413cea31e90d231da042e17683774f064f7fa359352e143c6fb6\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/75/01/e6c444034338bde9c7947d3467807f889123465c2371e77418\n",
      "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470949 sha256=b6e5f4f751e4900597c877ce37ccc529863185b59f352ff01572896912fe83a6\n",
      "  Stored in directory: /root/.cache/pip/wheels/0e/4a/ed/9653bc799915f52dce3f04d14946fbd85cce9c3cdedc9cfa71\n",
      "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6201 sha256=46a6f589581e29744687ffcf945c54f32cb854bd6bb3747e042975f7d228038f\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/67/e4/ba2159853bdd0fe99330aa1e384915108143a5370686ea446f\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=ddb49148789fe940bf1d761f6a5b259c0f47c6cd9c4f30ed297657f29de4a76b\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/b8/ba/ebba30390fbd997074f35e42a842ce3fd933213cac8753414e\n",
      "Successfully built docx2txt python-pptx compressed-rtf olefile\n",
      "Installing collected packages: SpeechRecognition, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, six, pycryptodome, olefile, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: xlrd\n",
      "    Found existing installation: xlrd 2.0.1\n",
      "    Uninstalling xlrd-2.0.1:\n",
      "      Successfully uninstalled xlrd-2.0.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.11.2\n",
      "    Uninstalling beautifulsoup4-4.11.2:\n",
      "      Successfully uninstalled beautifulsoup4-4.11.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
      "yfinance 0.2.13 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SpeechRecognition-3.8.1 XlsxWriter-3.0.9 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.17 python-pptx-0.6.21 six-1.12.0 textract-1.6.5 xlrd-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install fuzzywuzzy\n",
    "!pip install transformers\n",
    "!pip install textract\n",
    "import json\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import string\n",
    "# normalize nested JSON files\n",
    "from pandas.io.json import json_normalize\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2, urllib.request , nltk , textract\n",
    "from io import BytesIO\n",
    "import json\n",
    "#import weasyprint\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "from fuzzywuzzy import fuzz\n",
    "import json\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import regex\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPjJhDYty_S3",
    "outputId": "f596fb2a-7930-4b81-f4b8-fd64a8b03243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting html2text\n",
      "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: html2text\n",
      "Successfully installed html2text-2020.1.16\n"
     ]
    }
   ],
   "source": [
    "!pip install html2text\n",
    "import urllib.request as urllib2\n",
    "import html2text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J_AQXAT_-mk",
    "outputId": "1483a7cf-6695-4cbe-88ac-2f1928f03c13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfr5FHfo_47x"
   },
   "outputs": [],
   "source": [
    "json_file_path = \"/content/drive/MyDrive/RAjobs/OSU-Out-with-Thumbnails.json\"\n",
    "\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlGFxfLvuDit",
    "outputId": "5695296e-f155-4f22-84e5-621f49956394"
   },
   "outputs": [],
   "source": [
    "len(data), data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKOpt6GM_8Ei"
   },
   "outputs": [],
   "source": [
    "links = [x['link'] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LKLeWbDhVFC"
   },
   "outputs": [],
   "source": [
    "links = pd.DataFrame(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "RdcK6_yadhkp",
    "outputId": "f18efabd-dbd0-4660-cb89-a8910e824dc9"
   },
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qtbl3zZqeew4"
   },
   "outputs": [],
   "source": [
    "USDA = []\n",
    "\n",
    "def RequestHTML(url):\n",
    "  response = requests.get(url)\n",
    "  return response.text\n",
    "\n",
    "TEST = []\n",
    "N = 1546\n",
    "# usda_count = np.zeros([N,1])\n",
    "usda_count = []\n",
    "for i in range(N):\n",
    "  USDA.append([])\n",
    "  usda_count.append(0)\n",
    "# for i in range(N):\n",
    "  url_to_scrape = links.iloc[i,0]\n",
    "  html_document = RequestHTML(url_to_scrape)\n",
    "  soup = BeautifulSoup(html_document, \"html.parser\")\n",
    "  content = []\n",
    "  for link in soup.find_all('a', attrs={'href': re.compile(\"http\")}):\n",
    "    content.append(link.get('href', []))\n",
    "    if ('usda' in link.get('href', [])) and (not 'extension' in link.get('href',[])):\n",
    "        usda_count[i] += 1\n",
    "        USDA[i].append(link.get('href', []))\n",
    "\n",
    "  content = list(filter(None,content))\n",
    "  TEST.append(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69hfEGAp_vGG",
    "outputId": "65c40fb1-dffa-4508-fec2-23db880f9581"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 1546, 1546)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content), len(USDA), len(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6iO0BU2aClM"
   },
   "outputs": [],
   "source": [
    "links['usda_count'] = np.nan\n",
    "links['usda_link'] = np.nan\n",
    "#links['usda_count'].iloc[:30] = usda_count\n",
    "links['usda_count'] = usda_count\n",
    "links['usda_link'] = USDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8yuC8o3DkdA"
   },
   "outputs": [],
   "source": [
    "links.to_excel('/content/drive/MyDrive/RAjobs/links.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4oDmlZQooka"
   },
   "outputs": [],
   "source": [
    "#This one scrape all hyperlinks\n",
    "link1 = []\n",
    "for i in links:\n",
    "    url = i\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup= BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "        link1.append(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hav0RQUrB0E-"
   },
   "outputs": [],
   "source": [
    "urls = links[0]\n",
    "grab = requests.get(urls)\n",
    "soup = BeautifulSoup(grab.text, 'html.parser')\n",
    "\n",
    "# opening a file in write mode\n",
    "f = open(\"test1.txt\", \"w\")\n",
    "# traverse paragraphs from soup\n",
    "for link in soup.find_all(\"a\"):\n",
    "   data = link.get('href')\n",
    "   f.write(data)\n",
    "   f.write(\"\\n\")\n",
    "\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWTzvQrl_9gc"
   },
   "outputs": [],
   "source": [
    "# This one I exclude pngs\n",
    "pdf = []\n",
    "for i in links:\n",
    "    url = i\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup= BeautifulSoup(response.text, \"html.parser\")\n",
    "    j = 0\n",
    "\n",
    "\n",
    "\n",
    "    for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "        content=[]\n",
    "        content.append(link.get('href', []))\n",
    "        for urls in content:\n",
    "            if ('.pdf' in urls):\n",
    "                j+=1\n",
    "                pdf.append(link.get('href'))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        #link1.append(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3U4BFamj9Xw1"
   },
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(pdf)\n",
    "pdf.to_excel('/content/drive/MyDrive/RAjobs/OSU_pdfs.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qx0uUGak0OCA"
   },
   "outputs": [],
   "source": [
    "\n",
    "PDF = pdf.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjR2hfBi0Uc4"
   },
   "outputs": [],
   "source": [
    "agpr = []\n",
    "fmla = []\n",
    "land = []\n",
    "fnlo = []\n",
    "fpex = []\n",
    "fmpc = []\n",
    "crop= []\n",
    "grst = []\n",
    "acrg = []\n",
    "cropan = []\n",
    "prog = []\n",
    "hgpg = []\n",
    "cofd = []\n",
    "cost = []\n",
    "mkpr = []\n",
    "brls = []\n",
    "catl = []\n",
    "lstk = []\n",
    "dary = []\n",
    "ckeg = []\n",
    "hony = []\n",
    "s = 0\n",
    "e = 0\n",
    "f = 0\n",
    "for urls in PDF:\n",
    "  if ('agpr' in urls):\n",
    "    s+=1\n",
    "    agpr.append(link.get('href'))\n",
    "  else:\n",
    "    if ('fmla' in urls):\n",
    "      e+=1\n",
    "      fmla.append(link.get('href'))\n",
    "    else:\n",
    "      if ('land08' in urls) or ('AgriLandVa' in urls):\n",
    "        f+=1\n",
    "        land.append(link.get('href'))\n",
    "      else:\n",
    "        if ('fnlo' in urls) or ('FarmLandIn' in urls):\n",
    "          fnlo.append(link.get('href'))\n",
    "        else:\n",
    "          if ('fpex' in urls) or ('FarmProdEx' in urls):\n",
    "            fpex.append(link.get('href'))\n",
    "          else:\n",
    "            if ('fmpc' in urls) or ('FarmComp' in urls):\n",
    "              fmpc.append(link.get('href'))\n",
    "            else:\n",
    "              if ('crop' in urls):\n",
    "                crop.append(link.get('href'))\n",
    "              else:\n",
    "                if ('grst' in urls):\n",
    "                  grst.append(link.get('href'))\n",
    "                else:\n",
    "                  if ('acrg' in urls) or ('Acre' in urls):\n",
    "                    acrg.append(link.get('href'))\n",
    "                  else:\n",
    "                    if ('CropProdSu' in urls):\n",
    "                      cropan.append(link.get('href'))\n",
    "                    else:\n",
    "                      if ('prog' in urls):\n",
    "                        prog.append(link.get('href'))\n",
    "                      else:\n",
    "                        if ('hgpg' in urls):\n",
    "                          hgpg.append(link.get('href'))\n",
    "                        else:\n",
    "                          if ('cofd' in urls):\n",
    "                            cofd.append(link.get('href'))\n",
    "                          else:\n",
    "                            if ('cost' in urls):\n",
    "                              cost.append(link.get('href'))\n",
    "                            else:\n",
    "                              if ('mkpr' in urls):\n",
    "                                mkpr.append(link.get('href'))\n",
    "                              else:\n",
    "                                if ('brls' in urls):\n",
    "                                  brls.append(link.get('href'))\n",
    "                                else:\n",
    "                                  if ('catl' in urls):\n",
    "                                    catl.append(link.get('href'))\n",
    "                                  else:\n",
    "                                    if ('lstk' in urls):\n",
    "                                      lstk.append(link.get('href'))\n",
    "                                    else:\n",
    "                                      if ('dary' in urls):\n",
    "                                        dary.append(link.get('href'))\n",
    "                                      else:\n",
    "                                        if ('ckeg' in urls):\n",
    "                                          ckeg.append(link.get('href'))\n",
    "                                        else:\n",
    "                                          if ('hony' in urls) or ('Hone' in urls):\n",
    "                                            hony.append(link.get('href'))\n",
    "                                          else:\n",
    "                                            pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xyx6H0vS91qV",
    "outputId": "e8c7be8c-e739-4766-ff80-6a2b44aaec45"
   },
   "outputs": [],
   "source": [
    "print(agpr)\n",
    "print(fmla)\n",
    "print(land)\n",
    "print(fnlo)\n",
    "print(fpex)\n",
    "print(fmpc)\n",
    "print(crop)\n",
    "print(grst)\n",
    "print(acrg)\n",
    "print(cropan)\n",
    "print(prog)\n",
    "print(hgpg)\n",
    "print(cofd)\n",
    "print(cost)\n",
    "print(mkpr)\n",
    "print(brls)\n",
    "print(catl)\n",
    "print(lstk)\n",
    "print(dary)\n",
    "print(ckeg)\n",
    "print(hony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "py9QPp4Vz9U-"
   },
   "outputs": [],
   "source": [
    "pdf.to_excel('/content/drive/MyDrive/RAjobs/OKU_pdf.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gatj-ofPMx-A"
   },
   "outputs": [],
   "source": [
    "# This one I exclude pngs\n",
    "png = []\n",
    "pdf = []\n",
    "extension = []\n",
    "video = []\n",
    "for i in links:\n",
    "  url = i\n",
    "  try:\n",
    "      response = requests.get(url)\n",
    "      soup= BeautifulSoup(response.text, \"html.parser\")\n",
    "      j = 0\n",
    "      p = 0\n",
    "      q = 0\n",
    "      f = 0\n",
    "      for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "          content=[]\n",
    "          content.append(link.get('href', []))\n",
    "          for urls in content:\n",
    "              if ('.png' in urls):\n",
    "                  j+=1\n",
    "                  png.append(link.get('href'))\n",
    "              else:\n",
    "                  if ('.pdf' in urls):\n",
    "                    p+=1\n",
    "                    pdf.append(link.get('href'))\n",
    "                  else:\n",
    "                    if ('extension' in urls):\n",
    "                      q+=1\n",
    "                      extension.append(link.get('href'))\n",
    "                    else:\n",
    "                      if (\"youtube\" in urls):\n",
    "                        f+=1\n",
    "                        video.append(link.get('href'))\n",
    "                      else:\n",
    "                        pass\n",
    "\n",
    "  except urllib.error.HTTPError:\n",
    "      pass\n",
    "\n",
    "        #link1.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tfmGYdE-x-F",
    "outputId": "9b1bd6be-5c38-4e3d-cfd7-9e437968a570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6195\n",
      "801\n",
      "19927\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(png))\n",
    "print(len(pdf))\n",
    "print(len(extension))\n",
    "print(len(video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKKTQmMvKbgE",
    "outputId": "912d00e2-e3a4-4178-dfc7-e35094bce176"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6195"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zu7pdRhJEB4w",
    "outputId": "32941a90-59c4-4014-8534-2f6566957379"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOuy8ri-RC3z"
   },
   "outputs": [],
   "source": [
    "usda = []\n",
    "\n",
    "\n",
    "\n",
    "for i in links:\n",
    "    url = i\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup= BeautifulSoup(response.text, \"html.parser\")\n",
    "    j = 0\n",
    "\n",
    "\n",
    "\n",
    "    for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "        content=[]\n",
    "        content.append(link.get('href', []))\n",
    "        for urls in content:\n",
    "            if ('usda' in urls):\n",
    "              if ('extension' in urls):\n",
    "                pass\n",
    "              else:\n",
    "                j+=1\n",
    "                usda.append(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odtpudPoMRhz",
    "outputId": "db7806cb-3f2b-4c52-a4a2-5dc06dc7bd01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(usda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1YhAXDLLcDA"
   },
   "outputs": [],
   "source": [
    "cornell = []\n",
    "\n",
    "\n",
    "\n",
    "for i in links:\n",
    "    url = i\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup= BeautifulSoup(response.text, \"html.parser\")\n",
    "    j = 0\n",
    "\n",
    "\n",
    "\n",
    "    for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "        content=[]\n",
    "        content.append(link.get('href', []))\n",
    "        for urls in content:\n",
    "            if ('cornell' in urls):\n",
    "              j+=1\n",
    "              cornell.append(link.get('href'))\n",
    "            else:\n",
    "              pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hroRiAKYmSFt"
   },
   "outputs": [],
   "source": [
    "usda_link = pd.DataFrame(usda)\n",
    "usda_link.to_csv('/content/drive/MyDrive/RAjobs/usda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spjCdhoMme6R"
   },
   "outputs": [],
   "source": [
    "nass = []\n",
    "\n",
    "\n",
    "\n",
    "for i in links:\n",
    "    url = i\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup= BeautifulSoup(response.text, \"html.parser\")\n",
    "    j = 0\n",
    "\n",
    "\n",
    "\n",
    "    for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "        content=[]\n",
    "        content.append(link.get('href', []))\n",
    "        for urls in content:\n",
    "            if ('usda' in urls):\n",
    "                if ('nass' in urls):\n",
    "                  j+=1\n",
    "                  nass.append(link.get('href'))\n",
    "            else:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCphAL2DzfdQ",
    "outputId": "7f739052-91b1-425c-a36d-fa7dfaa4980d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpxZWR1_znYx"
   },
   "outputs": [],
   "source": [
    "report = []\n",
    "\n",
    "\n",
    "\n",
    "for i in links:\n",
    "    url = i\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup= BeautifulSoup(response.text, \"html.parser\")\n",
    "    j = 0\n",
    "\n",
    "\n",
    "\n",
    "    for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "        content=[]\n",
    "        content.append(link.get('href', []))\n",
    "        for urls in content:\n",
    "            if ('report' in urls):\n",
    "\n",
    "                j+=1\n",
    "                report.append(link.get('href'))\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpCXm4pUkrF2",
    "outputId": "b15201ea-2cee-47d7-b31f-5107483b595f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g4ANjCi-xz2D",
    "outputId": "c8e1749e-a57c-4fbb-cd82-79c04d4f1b9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69845"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE4zvI7Ix4fd"
   },
   "outputs": [],
   "source": [
    "\n",
    "extension = []\n",
    "\n",
    "\n",
    "\n",
    "for i in links:\n",
    "    url = i\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup= BeautifulSoup(response.text, \"html.parser\")\n",
    "    j = 0\n",
    "\n",
    "\n",
    "\n",
    "    for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "        content=[]\n",
    "        content.append(link.get('href', []))\n",
    "        for urls in content:\n",
    "            if ('extension' in urls):\n",
    "\n",
    "                j+=1\n",
    "                extension.append(link.get('href'))\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFHIcb9pHrTe"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in link1:\n",
    "    try:\n",
    "      page = urllib2.urlopen(i)\n",
    "    except urllib.error.URLError:\n",
    "        continue\n",
    "    except ConnectionError:\n",
    "        continue\n",
    "    html_content = page.read()\n",
    "    file = open('file_text.txt', 'wb')\n",
    "    file.write(html_content)\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
